import React, { useEffect, useRef, useState } from 'react';

const FrameFilterComponent: React.FC = () => {
  const videoRef = useRef<HTMLVideoElement | null>(null);
  const [badTimestamps] = useState<number[]>([1000000, 2000000, 3000000]); // Example timestamps in microseconds
  const [filteredStream, setFilteredStream] = useState<MediaStream | null>(null);

  useEffect(() => {
    const processStream = async () => {
      try {
        // 1. Get user media stream (video only for example)
        const mediaStream = await navigator.mediaDevices.getUserMedia({ video: true });
        const [videoTrack] = mediaStream.getVideoTracks();

        // 2. Create processor and generator
        // @ts-ignore
        const processor = new MediaStreamTrackProcessor({ track: videoTrack });
        const reader = processor.readable.getReader();
        // @ts-ignore
        const generator = new MediaStreamTrackGenerator({ kind: 'video' });
        const writer = generator.writable.getWriter();

        // 3. Start filtering loop
        const filterFrames = async () => {
          while (true) {
            const result = await reader.read();
            if (result.done) {
              console.log('No more frames.');
              writer.close();
              break;
            }

            const frame = result.value as VideoFrame;
            const frameTimestamp = frame.timestamp; // in microseconds

            if (badTimestamps.includes(frameTimestamp)) {
              console.log(`Dropping frame at timestamp: ${frameTimestamp}`);
              frame.close();
              continue;
            }

            await writer.write(frame);
            frame.close();
          }
        };

        filterFrames();

        // 4. Create filtered MediaStream from generator
        const filteredStream = new MediaStream();
        // @ts-ignore
        filteredStream.addTrack(generator);

        // Set to state
        setFilteredStream(filteredStream);
      } catch (error) {
        console.error('Error in processing stream:', error);
      }
    };

    processStream();
  }, [badTimestamps]);

  useEffect(() => {
    if (videoRef.current && filteredStream) {
      videoRef.current.srcObject = filteredStream;
      videoRef.current.play();
    }
  }, [filteredStream]);

  return (
    <div>
      <h2>Filtered Video Stream</h2>
      <video ref={videoRef} width="640" height="480" autoPlay muted></video>
    </div>
  );
};

export default FrameFilterComponent;







import React, { useEffect, useRef, useState } from 'react';

// === Paste this component into your codebase ===
const DEVIATION_WINDOW_MS = 2000;    // Keep deviation events from last 2 seconds
const DROP_THRESHOLD_US = 100_000;   // 100ms threshold for dropping frames

const VideoProcessor = () => {
  const videoRef = useRef<HTMLVideoElement>(null);
  const deviationTimestamps = useRef<number[]>([]);
  const processingController = useRef<AbortController>();
  const [isProcessing, setIsProcessing] = useState(false);
  const [error, setError] = useState<string | null>(null);

  // Call this function whenever deviation exceeds your threshold
  // For example: reportDeviation(Date.now());
  const reportDeviation = (timestampMs: number) => {
    const now = Date.now();
    const microTimestamp = timestampMs * 1000; // Convert ms to Î¼s
    // Keep only recent deviation timestamps
    deviationTimestamps.current = [
      ...deviationTimestamps.current.filter(ts =>
        ts > (now - DEVIATION_WINDOW_MS) * 1000
      ),
      microTimestamp
    ];
  };

  // Expose reportDeviation for your tracking logic
  useEffect(() => {
    (window as any).reportDeviation = reportDeviation;
  }, []);

  useEffect(() => {
    let processedStream: MediaStream | null = null;

    const startProcessing = async () => {
      try {
        const rawStream = await navigator.mediaDevices.getUserMedia({ video: true });
        const videoTrack = rawStream.getVideoTracks()[0];
        if (!videoTrack) throw new Error('No video track found');

        const processor = new (window as any).MediaStreamTrackProcessor({ track: videoTrack });
        const generator = new (window as any).MediaStreamTrackGenerator({ kind: 'video' });
        processedStream = new MediaStream([generator]);
        if (videoRef.current) videoRef.current.srcObject = processedStream;

        processingController.current = new AbortController();
        setIsProcessing(true);

        const reader = processor.readable.getReader();
        const writer = generator.writable.getWriter();

        const processFrames = async () => {
          while (!processingController.current?.signal.aborted) {
            const { done, value: frame } = await reader.read();
            if (done || !frame) break;

            // Drop frame if close to any deviation timestamp
            const shouldDrop = deviationTimestamps.current.some(devTs =>
              Math.abs(devTs - frame.timestamp) < DROP_THRESHOLD_US
            );

            if (!shouldDrop) {
              await writer.write(frame);
            }
            frame.close();
            // Yield to event loop
            await new Promise(resolve => requestAnimationFrame(resolve));
          }
        };

        processFrames();
      } catch (err: any) {
        setError(err.message || 'Video processing failed');
        setIsProcessing(false);
      }
    };

    startProcessing();

    return () => {
      processingController.current?.abort();
      if (processedStream) {
        processedStream.getTracks().forEach(track => track.stop());
      }
      setIsProcessing(false);
    };
  }, []);

  return (
    <div>
      <video ref={videoRef} autoPlay muted playsInline style={{ width: 480, height: 320, background: '#000' }} />
      <div>Status: {isProcessing ? 'Processing' : 'Stopped'}</div>
      {error && <div style={{ color: 'red' }}>Error: {error}</div>}
      <div style={{ marginTop: 8, fontSize: 12, color: '#888' }}>
        Call <code>window.reportDeviation(Date.now())</code> in your deviation logic to drop frames!
      </div>
    </div>
  );
};

export default VideoProcessor;














import React, { useEffect, useRef, useState } from 'react';

const DEVIATION_WINDOW_MS = 2000;    // Keep deviation events from last 2 seconds
const DROP_THRESHOLD_US = 100_000;   // 100ms threshold for dropping frames

const VideoProcessor = () => {
  const videoRef = useRef<HTMLVideoElement>(null);
  const deviationTimestampsUs = useRef<number[]>([]);
  const controllerRef = useRef<AbortController | null>(null);
  const [isProcessing, setIsProcessing] = useState(false);
  const [error, setError] = useState<string | null>(null);

  // Report deviation (called externally)
  const reportDeviation = (timestampMs: number) => {
    const nowUs = Date.now() * 1000;
    const deviationUs = timestampMs * 1000;

    deviationTimestampsUs.current = [
      ...deviationTimestampsUs.current.filter(ts => ts > nowUs - DEVIATION_WINDOW_MS * 1000),
      deviationUs,
    ];
  };

  // Expose reportDeviation globally
  useEffect(() => {
    (window as any).reportDeviation = reportDeviation;
    return () => {
      delete (window as any).reportDeviation;
    };
  }, []);

  useEffect(() => {
    let processedStream: MediaStream | null = null;

    const startProcessing = async () => {
      setIsProcessing(true);
      setError(null);

      const abortController = new AbortController();
      controllerRef.current = abortController;

      try {
        const rawStream = await navigator.mediaDevices.getUserMedia({ video: true });
        const videoTrack = rawStream.getVideoTracks()[0];
        if (!videoTrack) throw new Error('No video track found');

        const processor = new (window as any).MediaStreamTrackProcessor({ track: videoTrack });
        const generator = new (window as any).MediaStreamTrackGenerator({ kind: 'video' });
        processedStream = new MediaStream([generator]);

        if (videoRef.current) {
          videoRef.current.srcObject = processedStream;
          await videoRef.current.play().catch(() => {});
        }

        const reader = processor.readable.getReader();
        const writer = generator.writable.getWriter();

        const processFrames = async () => {
          try {
            while (true) {
              const { done, value: frame } = await reader.read();
              if (done || abortController.signal.aborted) break;
              if (!frame) continue;

              const timestamp = frame.timestamp;

              const shouldDrop = deviationTimestampsUs.current.some(
                devTs => Math.abs(devTs - timestamp) < DROP_THRESHOLD_US
              );

              if (!shouldDrop) {
                await writer.write(frame);
              }

              frame.close();
            }
          } finally {
            try {
              await writer.close();
            } catch {}
            try {
              reader.releaseLock();
            } catch {}
          }
        };

        processFrames();
      } catch (err: any) {
        console.error('Video processing error:', err);
        setError(err?.message || 'Video processing failed');
      } finally {
        setIsProcessing(false);
      }
    };

    startProcessing();

    return () => {
      controllerRef.current?.abort();
      processedStream?.getTracks().forEach(track => track.stop());
      setIsProcessing(false);
    };
  }, []);

  return (
    <div>
      <video
        ref={videoRef}
        autoPlay
        muted
        playsInline
        style={{ width: 480, height: 320, background: '#000' }}
      />
      <div>Status: {isProcessing ? 'Processing' : 'Stopped'}</div>
      {error && <div style={{ color: 'red' }}>Error: {error}</div>}
      <div style={{ marginTop: 8, fontSize: 12, color: '#888' }}>
        Call <code>window.reportDeviation(Date.now())</code> in your deviation logic to drop frames!
      </div>
    </div>
  );
};

export default VideoProcessor;






async function getFirstFrameTimestamp(track) {
  const processor = new MediaStreamTrackProcessor({ track });
  const reader = processor.readable.getReader();
  
  try {
    const { value: firstFrame } = await reader.read();
    const firstTimestamp = firstFrame.timestamp;
    firstFrame.close(); // Don't forget to close the frame
    return firstTimestamp;
  } finally {
    reader.releaseLock();
    // Processor will be garbage collected
  }
}

// Usage:
const videoTrack = stream.getVideoTracks()[0];
getFirstFrameTimestamp(videoTrack).then(timestamp => {
  console.log("First frame timestamp:", timestamp);
});
